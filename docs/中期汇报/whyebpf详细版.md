[TOC]

# why ebpf

## 分布式文件系统的主要IO操作

1. 读取文件数据：读取指定文件的内容，包括所有的数据块。

2. 写入文件数据：将指定文件的内容写入磁盘，包括所有的数据块。

3. 元数据操作：在创建、删除、重命名和截断文件等操作时需要配置文件的所有元数据，如文件名、大小和创建日期等信息。

## 提高分布式文件系统IO性能的思路

1. 网络IO优化：分布式文件系统通常是基于网络架构实现的，因此网络IO对整个系统的性能影响较大。可以采取如下措施进行网络IO优化：使用高性能网络设备，如RDMA网卡、高速交换机、路由器等；采用分布式存储技术，将数据分散到多个节点上存储，降低单个节点的IO负载；使用协议优化，如TCP加速、UDP卸载等；

2. 存储设备优化：分布式文件系统维护了大量的数据，因此存储设备的性能对整个系统的性能影响很大。可以通过如下措施进行存储设备优化：使用高速SSD设备；采用数据分布技术，将数据分散到多个磁盘上存储；使用RAID技术提升数据读写性能；

3. 数据访问优化：分布式文件系统是基于网络的，而数据对于应用程序的访问则是通过关键的数据访问路径来实现的。可以通过如下措施进行数据访问优化：采用数据预读技术，提前将数据从存储设备读入到缓存中，提升数据读取效率；进行缓存优化，如使用多级缓存、采用适当的缓存淘汰策略等；使用数据分区技术，将数据分区存储，并针对不同的数据访问类型采用不同的存储策略；

4. 系统架构优化：设计和选择适合的分布式文件系统架构，可以有效地提升IO性能。可以采用如下措施进行系统架构优化：采用分布式计算框架，如Hadoop、Spark等；使用分布式文件系统中心化管理架构，降低系统复杂度；采用分布式文件系统的扩展性能力，可以随着数据量的增加横向扩展系统性能；

5. 内存管理优化：内存管理对于文件系统的性能也十分关键。可以通过使用高速内存、优化系统内存参数以及避免内存碎片等方式来优化内存管理性能。

6. 引入新技术：例如eBPF和XDP技术，可以在内核中优化文件系统的IO操作，提高文件系统的性能。

## 传统linuxIO过程

![](usefulpics/传统IO.jpg)

通过 read() 系统调用读取文件到到内核缓冲区，通过 write() 系统调用把内核缓冲区中的数据输出到网络端口。在用户程序向内核发起系统调用及调用返回时，会产生用户态与进程态之间的上下文切换；在各个缓冲区之间的数据拷贝由CPU及DMA控制器来完成。

可优化之处：
减少用户态与内核态间的上下文切换
减少各缓冲区之间的拷贝次数

## 现有可加速IO的系统调用函数

![](usefulpics/mmap.png)

- mmap：建立内核读缓冲区与用户空间缓冲区的虚拟内存映射，使得用户进程可以直接访问文件内容而不需要通过系统调用，从而省去了一次CPU拷贝，可提升文件读性能；但无法优化写性能，且内存映射总是要求页边界对齐，可能会造成内存浪费，在文件较大时内存映射空间可能不足。

![](usefulpics/sendfile.jpg)

- sendfile：数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝；但在内核缓冲区与网络缓冲区之间的数据拷贝不可避免，且sendfile 调用中 I/O 数据对用户空间是完全不可见的，无法进行更细粒度的文件操作。

![](usefulpics/splice.jpg)

- splice：在内核缓冲区和网络缓冲区之间建立管道，既省去了数据在用户空间和内核空间之间的来回拷贝，又避免了内核缓冲区和网络缓冲区之间的 CPU 拷贝操作；但splice 拷贝方式也同样存在用户程序不能对数据进行修改的问题。

## 现有可加速网络数据包处理的相关技术

### DPDK

DPDK使用了Kernel Bypass的设计思想，即避免应用程序使用网络协议栈和操作系统内核提供的函数来处理网络数据包，而是直接访问网络适配器的硬件资源，通过直接驱动网卡完成数据包的发送和接收，减少了CPU的上下文切换和处理过程，从而提高了数据包处理速度和吞吐量。
但由于避免了操作系统和库函数的调用，在DPDK中应用程序需要自己实现一部分操作系统中的功能，提供特定的API接口，开发难度较高；由于DPDK绕过了操作系统内核提供的网络协议栈，系统管理和维护有一定的难度；DPDK技术对网络适配器的要求比较高，需要适配器硬件支持并且需要定制化驱动程序；由于绕过了一些操作系统中的底层网络功能（如防火墙、安全策略等），在某些情况下会降低数据可靠性和安全性。

综上所述，DPDK技术虽然可以提高网络性能，但其高成本、高难度和潜在的可靠性和安全性问题，限制了其在应用程序中的广泛应用。

### XDP

![](pics/xdp架构.jpg)
XDP（eXpress Data Path）可以通过在千兆/万兆网卡驱动程序中运行eBPF程序来提高网络流量的处理速度，从而实现数据包处理网络加速。具体而言，XDP可以在网卡接收和发出数据包的前后处理阶段运行eBPF程序来对网卡数据流量进行优化、过滤和预处理。这些eBPF程序可以通过hook函数来与网卡驱动程序和内核协同工作，以实现更高效的数据包传输和流量控制。
XDP技术是ebpf在网络数据包处理方面的具体应用，可以直接在内核态进行数据包的处理和过滤，避免用户态和内核态之间的频繁切换的开销，提高数据处理的效率。但XDP并没有提供与用户空间的直接交互方式，需要通过额外的机制来实现与应用程序的交互，比如将数据复制到共享内存区域中；其仅适用于数据包处理，并且只能在网络接口的驱动程序中使用。

## 现有可提高存储与传输效率的相关技术

### 缓存技术

如Memcached和Redis等高性能缓存系统。这些系统可以将热数据缓存到内存中，从而加快对数据的访问速度，提高系统性能。

### 分布式存储技术

如Hadoop、Cassandra等分布式存储系统。这些系统可以水平扩展存储能力，提高数据吞吐量，同时保障数据的安全性和可靠性。

### SPDK技术

SPDK（Storage Performance Development Kit）技术采用了用户空间驱动程序的设计方法，通过一些特定的API，如NVMe协议栈的API，用户程序可直接访问存储设备地址空间和DMA引擎，绕过了操作系统内核的复杂处理流程，从而尽可能减少内核空间与用户空间之间的数据传输和上下文切换，实现更高的存储I/O性能。但SPDK 技术开发门槛较高，且绕过操作系统内核会导致管理和维护困难。

### XRP技术

![](pics/xrp架构.png)
XRP（In-Kernel Storage Functions with eBPF）是一种利用eBPF实现的内核层技术，主要用于提高存储和传输的效率。与 SPDK 完全绕开内核存储栈，采⽤polling的⽅式来访问存储的⽅式不同，XRP 则是借鉴XDP的实现思路，将中间请求直接在 NVMe 驱动层进⾏重提交，从⽽避免让中间请求通过冗⻓的存储栈后再提交，达到加速的⽬的。在使⽤ XRP 的存储访问⽅式中，只有第⼀次请求和最后⼀次响应会经过⼀个完整的存储栈路径。XRP技术是近两年内提出的，技术发展尚未成熟，也并未与分布式文件系统相结合。

## ebpf的优势

![](usefulpics/ebpf参与的IO.png)

- ebpf map可以减少数据拷贝次数
- ebpf可以减少用户态与内核态之间的上下文切换
- ebpf可以利用各种内核缓冲区
- ebpf的安全检测机制可以提高程序运行的安全性
- ebpf允许在加速文件传输的同时对文件进行细粒度的操作
- eBPF可以实现内存缓存和预读机制
- eBPF程序可以在运行时动态修改
- eBPF数据可跨平台（如Prometheus，Grafana）使用

### ebpf map可以减少数据拷贝次数

eBPF map可以在用户态和内核态之间提供高速的通信桥梁,数据可以在内核空间和用户空间中共享，避免了数据的多次拷贝，从而提高了程序的处理效率。此外，ebpf map 还可以用于记录文件系统 IO 操作的元数据，这些元数据可以在多个内核函数中共享，从而避免重复计算，提高文件系统 IO 的效率。

### ebpf可以减少用户态与内核态之间的上下文切换

eBPF是一种在 Linux 内核中执行的实时编译器，它可以根据用户定义的过滤器实时过滤和修改内核中的数据。当用户程序调用 eBPF 程序时，eBPF 程序以内核二进制代码的形式被加载到内核中，此后通过类似JIT的方式在内核中执行，因此它不需要进行与用户态和内核态之间频繁地切换上下文。

### ebpf的安全检测机制可以提高程序运行的安全性

eBPF有一些安全检测机制，可以提高程序运行的安全性。这些机制主要包括：

1. eBPF程序必须经过内核的审核和验证。eBPF程序必须符合特定的规则才能够在内核中运行，否则会被拒绝执行。

2. eBPF程序只能够访问特定的内存区域。eBPF程序只能够访问自己的内存区域以及内核定义的特定的内存区域。

3. eBPF程序不能直接调用内核的系统调用。eBPF程序只能够通过eBPF提供的特定接口来与内核进行交互，并遵循内核提供的访问控制规则。

### eBPF可以实现内存缓存和预读机制

ebpf程序中提供了一些库函数可以实现内存缓存和预读

- bpf_map_lookup_elem: 这个函数可以通过给定的键查找BPF映射表并返回相关的值。例如，可以使用这个函数从BPF映射表中查找文件数据块并将其读入用户空间缓存中。

- bpf_map_update_elem: 这个函数可以通过给定的键更新BPF映射表中的值。例如，可以使用这个函数更新缓存中的文件数据块以及相关的元数据信息。

- bpf_skb_load_bytes: 这个函数可以从内核空间缓存中读取指定长度的数据到指定的内存缓存中。例如，可以使用这个函数将从网络传输中收到的数据块读入内存缓存中。

- bpf_skb_store_bytes: 这个函数可以将指定长度的数据从内存缓存中写入内核空间中。例如，可以使用这个函数将缓存中的文件数据块写回到文件系统中。

- bpf_prefetch: 这个函数可以预取指定地址到CPU的缓存中，以便之后的访问可以快速地从缓存中读取。例如，可以使用这个函数在访问指定内存地址之前预先将它们加载到CPU缓存中，以加速IO操作。

此外，eBPF Map也可以用于实现内存缓存和预读机制

- 映射表存储缓存数据：
可以使用eBPF Map存储文件数据块，再通过键值对的方式从内存缓存中读写数据，减少磁盘IO和相关数据访问，从而达到加速数据读取和降低IO负载的目的。

- 映射表实现预读机制：
可以使用eBPF Map实现预读机制，当某个数据块访问次数较多时，将其提前读到内存缓存中。同时计算访问频率去施加策略，安排预读缓存的大小，并通过eBPF Map保存和存储每个预读次数阈值所对应的缓存块。可以根据预读次数阈值，将数据缓存到eBPF Map中，以便后续快速访问。

### eBPF程序可以在运行时动态修改

ebpf可运行时动态修改包括两个方面的含义：

1. 动态修改eBPF程序代码：eBPF程序可以在运行时动态修改其代码，从而可以基于实际情况对其进行调整和优化。例如，我们可以根据IO操作的类型和特征，对eBPF程序进行调整和优化，从而提高其响应速度和吞吐量。

2. 动态修改eBPF程序的映射表：eBPF程序通常会与用户空间的映射表交互，在运行时可以动态修改映射表中的键值对。例如，对于JuiceFS中的IO操作，我们可以通过向eBPF程序传递特定的参数，以实现对映射表中的键值对进行动态修改，从而进一步优化IO操作的性能。

运行时动态修改eBPF程序代码和映射表有利于减少系统维护成本和提高系统的可维护性。例如，如果发现IO操作中出现性能瓶颈，管理员可以通过修改eBPF程序代码和映射表，而不需要停止服务或重新启动系统，从而减少了系统的维护成本和停机时间。

### ebpf可以利用各种内核缓冲区

与kernal bypass的实现方式不同，eBPF程序在运行时不会绕过内核中的所有缓冲区，而是依赖于内核中的缓存机制和相关的内存访问权限。
linux系统的IO过程中存在的各种缓冲区：在Linux中，缓冲区的内存可以是由文件系统或者网络协议栈等子系统管理的，也可以是由设备驱动程序自己申请和管理的。文件系统管理的缓冲区通常指的是页缓存（Page Cache），它位于内核中，并由内核动态管理。当应用程序访问文件时，文件的内容会被载入到页缓存中，以便以后更快地访问，当写入文件时，数据会在页缓存中被缓存，这样可以降低频繁写入磁盘对磁盘性能产生的影响。网络协议栈管理的缓冲区一般是指套接字缓冲区（Socket Buffer，简称 skb），它是对网络数据包的抽象表示，当网络数据包从网络设备接收到时，它会被封装成 skb，并交给协议栈进行处理，当需要发送数据时，应用程序将数据发送到套接字缓冲区中，并交给协议栈发送，协议栈会再将其封装成 skb 进行发送。设备驱动程序申请的缓冲区通常是指DMA缓冲区（Direct Memory Access Buffer），也称为设备缓冲区。DMA 缓冲区是物理内存区域的一部分，用于在设备和内存之间传输数据。DMA 缓冲区是由设备驱动程序负责管理的，驱动程序需要向内核申请 DMA 缓冲区的空间，以确保读写能够顺利进行。在使用完毕后，驱动程序还需要负责释放这些缓冲区，以便内存使用效率最大化。
