# 调研报告

[TOC]

-----Made by ys

- [调研报告](#调研报告)
  - [项目概述](#项目概述)
  - [项目背景](#项目背景)
    - [一. 分布式图文件系统](#一-分布式图文件系统)
    - [1. **文件系统**：](#1-文件系统)
    - [2. **分布式文件系统**：](#2-分布式文件系统)
    - [**优点**：可扩展、高可用性、低成本](#优点可扩展高可用性低成本)
    - [**缺点** ：](#缺点-)
    - [3. **分布式图文件系统**(**DisGraFS**)：](#3-分布式图文件系统disgrafs)
    - [二. eBPF 介绍](#二-ebpf-介绍)
    - [1. eBPF 可以做什么？](#1-ebpf-可以做什么)
    - [2. eBPF 如何进行工作？](#2-ebpf-如何进行工作)
    - [内核的 eBPF 校验器](#内核的-ebpf-校验器)
    - [eBPF 数据结构](#ebpf-数据结构)
    - [eBPF 辅助函数](#ebpf-辅助函数)
    - [三. 分布式系统中的复制](#三-分布式系统中的复制)
      - [确保不同副本之间的状态一致的方法](#确保不同副本之间的状态一致的方法)
    - [四. DMA与零拷贝技术](#四-dma与零拷贝技术)
  - [立项依据](#立项依据)
  - [前瞻性/重要性分析](#前瞻性重要性分析)
  - [相关工作](#相关工作)
    - [ExtFUSE(使用ebpf优化FUSE性能)](#extfuse使用ebpf优化fuse性能)
    - [利用 eBPF 优化 K8s Service ](#利用-ebpf-优化-k8s-service-)
  - [可行性](#可行性)
  - [参考资料：](#参考资料)



## 项目概述

本项目基于 2021 年 OSH 项目 DisGraFS 展开，参考了2022年OSH的TOBEDONE小组和WowKiddy小组对DisGraFS进行的优化，尝试利用eBPF减少文件IO时的拷贝次数，对分布式图文件系统的性能进行进一步的优化提升。

---



## 项目背景

---



### 一. 分布式图文件系统

### 1. **文件系统**：

操作系统用于在磁盘或分区上组织文件的方法和数据结构。操作系统中负责**管理和存储文件信息的软件被称为文件管理系统**，简称文件系统。

### 2. **分布式文件系统**：

分布式文件系统（Distributed File System）是一种允许文件通过网络在多台主机上共享的文件系统，可以让多机器上的多用户进行文件分享和存储。客户端并非直接访问底层的数据存储区块，而是通过网络，以特定的通信协议和服务器沟通。

### **优点**：可扩展、高可用性、低成本

与此同时，不同DFS还有各自的优势，如Ceph同时提供的对象、块、文件存储功能，HDFS的高容错性等等。

### **缺点** ：

一. **索引方面：**

现有文件系统多数基于树形结构，设计面向计算机而非人的思维逻辑，不利于用户与现有文件系统的交互，随着文件系统的应用团队化，树形结构文件索引对人机交互将产生很大阻碍

二. [性能方面]([www.infoq.cn](https://www.infoq.cn/article/some-thinking-about-the-present-situation-and-future-of-ceph))(以Ceph为例）:**

a. **数据双倍写入**。Ceph 本地存储接口（FileStore）为了支持事务，引入了日志（Journal）机制。所有的写入操作都需要先写入日志（XFS 模式下），然后再写入本地文件系统。简单来说就是一份数据需要写两遍，日志 + 本地文件系统。这就造成了在大规模连续 IO 的情况下，实际上磁盘输出的吞吐量只有其物理性能的一半。

b. **IO 路径过长**。这个问题在 Ceph 的客户端和服务器端都存在。以 osd 为例，一个 IO 需要经过 message、OSD、FileJournal、FileStore 多个模块才能完成，每个模块之间都涉及到队列和线程切换，部分模块在对 IO 进行处理时还要进行内存拷贝，导致整体性能不高。

c. **对高性能硬件的支持有待改进**。Ceph 最开始是为 HDD 设计的，没有充分考虑全 SSD，甚至更先进的 PCIe SSD 和 NVRAM 的情况 NVRAM。导致这些硬件的物理性能在 Ceph 中无法充分发挥出来，特别是延迟和 IOPS，受比较大的影响。

### 3. **分布式图文件系统**(**DisGraFS**)： 

DisGraFS为改善上述问题，提出了分布式图文件系统的概念，统一了单机图文件系统和分布式文件系统的优点。将图结构与思想应用于分布式文件系统上面，使得分布式图文件系统兼具图文件系统方便用户快速搜索，模糊搜索，查找相关文件的特点以及分布式文件系统的海量文件存储，云存储的特点。同时吸取前人的经验，整个项目开发过程均会使用便于跨平台的语言和属性，以满足当今以及未来社会对于多设备，多平台的大容量，快搜索的文件系统的需求。

---



### 二. eBPF 介绍

[eBPF](https://lwn.net/Articles/740157/)是Linux内核中软件实现的虚拟机。用户把eBPF程序编译为eBPF指令，然后通过bpf()系统调用将eBPF指令加载到内核的特定挂载点，由特定的事件来触发eBPF指令的执行。在挂载eBPF指令时内核会进行充分验证，避免eBPF代码影响内核的安全和稳定性。另外内核也会进行JIT编译，把eBPF指令翻译为本地指令，减少性能开销。

内核在网络处理路径上中预置了很多eBPF的挂载点，例如xdp, qdisc, tcp-bpf, socket等。eBPF程序可以加载到这些挂载点，并调用内核提供的特定API来修改和控制网络报文。eBPF程序可以通过map数据结构来保存和交换数据。

### 1. eBPF 可以做什么？

一个 eBPF 程序会附加到指定的内核代码路径中，当执行该代码路径时，会执行对应的 eBPF 程序。鉴于它的起源，eBPF 特别适合编写网络程序，将该网络程序附加到网络 socket，进行流量过滤、流量分类以及执行网络分类器的动作。eBPF 程序甚至可以修改一个已建链的网络 socket 的配置。XDP 工程会在网络栈的底层运行 eBPF 程序，高性能地处理接收到的报文。从下图可以看到 eBPF 支持的功能：

![img](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/90e32d200bad7a69e65cf57f12a08424.png)

eBPF 对调试内核和执行性能分析也具有很大的帮助，程序可以附加到跟踪点、kprobes 和 perf 事件。因为 eBPF 可以访问内核数据结构，**开发者可以在不编译内核的前提下编写并测试代码**。对于工作繁忙的工程师，通过该方式可以方便地调试一个在线运行的系统。此外，还可以通过静态定义的追踪点调试用户空间的程序 (即 BCC 调试用户程序，如 Mysql)。

使用 eBPF 可以发挥其两大优势：**快速和安全**。

### 2. eBPF 如何进行工作？

**eBPF 程序是在内核中被事件触发的。**在一些特定的指令被执行时，这些事件会在 hook 处被捕获。Hook 被触发就会执行 eBPF 程序，对数据进行捕获和操作。接下来将系统介绍 eBPF 是如何工作的，你将了解到校验器流程、系统调用以及后续工作中所涉及到的程序类型、数据结构和辅助函数等内容。

### 内核的 eBPF 校验器

**在内核中运行用户空间的代码可能会存在安全和稳定性风险。因此，在加载 eBPF 程序前需要进行大量校验。**

![img](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/8bf3d70f514e3fde9b6bc6edb0d86f6b.png)

**首先通过对程序控制流的深度优先搜索保证 eBPF 能够正常结束，不会因为任何循环导致内核锁定**。严禁使用无法到达的指令；任何包含无法到达的指令的程序都会导致加载失败。

**第二个阶段涉及使用校验器模拟执行 eBPF 程序 (每次执行一个指令)**。在每次指令执行前后都需要校验虚拟机的状态，保证寄存器和栈的状态都是有效的。严禁越界 (代码) 跳跃，以及访问越界数据。

校验器不会检查程序的每条路径，它能够知道程序的当前状态是否是已经检查过的程序的子集。由于前面的所有路径都必须是有效的 (否则程序会加载失败)，当前的路径也必须是有效的，因此允许验证器“修剪”当前分支并跳过其模拟阶段。

校验器有一个 "安全模式"，禁止指针运算。当一个没有 CAP_SYS_ADMIN 特权的用户加载 eBPF 程序时会启用安全模式，确保不会将内核地址泄露给非特权用户，且不会将指针写入内存。如果没有启用安全模式，则仅允许在执行检查之后进行指针运算。例如，所有的指针访问时都会检查类型，对齐和边界冲突。

无法读取包含未初始化内容的寄存器，尝试读取这类寄存器中的内容将导致加载失败。R0-R5 的寄存器内容在函数调用期间被标记未不可读状态，可以通过存储一个特殊值来测试任何对未初始化寄存器的读取行为；对于读取堆栈上的变量的行为也进行了类似的检查，确保没有指令会写入只读的帧指针寄存器。

**最后，校验器会使用 eBPF 程序类型来限制可以从 eBPF 程序调用哪些内核函数，以及访问哪些数据结构**。例如，一些程序类型可以直接访问网络报文。

### eBPF 数据结构

**eBPF 使用的主要的数据结构是 eBPF map，这是一个通用的数据结构，用于在内核或内核和用户空间传递数据。**其名称 "map" 也意味着数据的存储和检索需要用到 key。

使用 bpf() 系统调用创建和管理 map。当成功创建一个 map 后，会返回与该 map 关联的文件描述符。关闭相应的文件描述符的同时会销毁 map。每个 map 定义了四个值：类型，元素最大数目，数值的字节大小，以及 key 的字节大小。eBPF 提供了不同的 map 类型，不同类型的 map 提供了不同的特性。

### eBPF 辅助函数

eBPF 程序被触发时，会调用辅助函数。这些特别的函数让 eBPF 能够有访问内存的丰富功能。

可以参考官方帮助文档查看 libbpf 库提供的辅助函数。 

官方文档给出了现有的 eBPF 辅助函数。更多的实例可以参见内核源码的 samples/bpf/ 和 tools/testing/selftests/bpf/ 目录。

---



### 三. [分布式系统中的复制](https://blog.csdn.net/jiafu1115/article/details/53908715)

复制可以分为同步复制和异步复制，两者的区别在于前者需要等待所有副本返回写入确认，而后者只需要一个返回确认即可。

从用途上，复制可以分为两类，一类用于确保不同副本的表现行为一致（避免divergence），另一类则用于允许不同副本之间的数据差异（divergence不可避免，如Dynomo）

####  确保不同副本之间的状态一致的方法

a. 主从复制

主从之间可以是异步复制，也可以是同步复制。例如MySQL，在默认情况下采用异步复制，异步复制容易引起数据丢失，比如主从结构中，主节点的写入请求还没有复制到从节点就出现故障，当从节点被选为新的主节点之后，在这之前写入没有同步的数据就会被丢失。即便采用了同步复制，也只能提供相对较弱的基本保障，考虑如下情形：主接收写入请求然后发到从节点，从节点写入成功后并发送确认给主，如果此时主节点正准备发送确认信息给客户端时出现故障，那么客户端就会认为提交失败，可是从节点已经提交成功了，如果这是从节点被提升为主，那么就出现问题了。

在主从复制结构里，异步复制相比同步复制具备更高的吞吐量和更低延迟，因此，结合同步和异步复制是一个常见选项。

b. 两阶段提交

例如MySQL Cluster就是通过2PC来提供数据同步的。相比之下，主从复制可以看做一阶段提交，因为没有发生错误的回滚。由于2PC很容易因为参与者宕机导致一直阻塞，因此2PC用于复制并不常见。

c. 分区一致性算法Paxos(复制状态机)

复制状态机在数据库开发的很多领域都可以遇到，比如Google Megastore，针对不同分区的每次提交采用复制状态机来确保每个分区的全局事务提交时序；Google Spanner在单分区内也采用了类似的设计。复制状态机主要用于满足两点需求：客户端在面对任何一个副本时都具备完全一致的访问行为；每个副本在执行请求时都需要按照完全一致的顺序来进行。

复制状态机通常都是基于复制日志实现的，每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。保证复制日志相同就是一致性算法的工作了。在一台服务器上，一致性模块接收客户端发送来的指令然后增加到自己的日志中去。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，尽管有些服务器会宕机。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成一个高可靠的状态机。

---



### 四. [DMA与零拷贝技术](https://www.cnblogs.com/xiaolincoding/p/13719610.html)



**DMA**是一种内存访问技术。DMA（Direct Memory Access，直接存储器访问）可以在不需要 CPU 参与的情况下实现内存的读取或写入，因为不依赖 CPU 的大量中断负载，因而可以实现数据的快速传送，提高系统的并发性能。

DMA 的传输过程必须经过 DMA 请求，DMA 响应，DMA 传输，DMA 结束 4 个步骤：

- DMA 请求：CPU 对 DMA 芯片进行设置，说明需要传送的字节数，有关的设备和内存地址，然后启动 DMA；
- DMA 响应：DMA 向 CPU 请求总线控制权，CPU 处理完当前总线数据后就让出总线；
- DMA 传输：DMA 控制器 直接控制内存与 I/O 接口进行数据传输；
- DMA 结束：DMA 传输结束后，把总线控制权交还给 CPU，并向 I/O 接口发送结束信号。

**零拷贝**指计算机不需要先将数据从一个内存区域复制到另外一个内存区域，从而减少系统调用切换、减少拷贝次数，从而减少 CPU 的执行时间和负载。



在没有 DMA 技术前，I/O 的过程是这样的：

- CPU 发出对应的指令给磁盘控制器，然后返回；
- 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个**中断**；
- CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。

![img](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/I_O%20%E4%B8%AD%E6%96%AD.png)





传统的 I/O 方式需要经过**四次**拷贝才能把磁盘上的数据输出到网络端口：

1. 执行 read 系统调用，从用户态切换到内核态，CPU 向 DMA 控制器芯片下发指令，将磁盘数据通过直接内存访问的方式拷贝到内核缓冲区中；
2. CPU 接收到 DMA 结束拷贝的信号，将内核缓冲区的数据拷贝到用户缓冲区中，read 调用结束，返回到用户态；
3. 用户程序执行 write 系统调用，从用户态切换到内核态，CPU 将数据从用户缓冲区中拷贝到Socket 发送缓冲区中；
4. CPU 下发指令，让 DMA 控制器来处理数据，将 Socket 发送缓冲区的数据拷贝到网卡进行网络传输，write 调用结束。

![img](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)

由此可以看出传统的文件传输开销很大，期间共**发生了 4 次用户态与内核态的上下文切换**，还**发生了 4 次数据拷贝**，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。



零拷贝有几种实现方式，如下：

- **mmap + write**：mmap 是一个系统调用，主要作用就是将用户缓冲区与内核中的读缓冲区进行映射，映射后这一步就不需要进行数据拷贝了，而 write 操作实际上是从内核读缓冲区中把数据拷贝到 Socket 发送缓冲区，整个过程减少了一次拷贝操作，但是系统调用切换没有减少。

![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20+%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)

- **sendfile**：sendfile 同样省去了将数据在内核和用户空间中拷贝，与 mmap 不同的是，sendfile 不需要借助 write 调用，而是一次完整的内核拷贝过程，减少了两次 CPU 上下文切换。
- ![img](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png)
- **sendfile + DMA gather copy**：对 sendfile 系统调用做了修改，引入了 gather 操作，不需要将内核缓冲区的数据拷贝到 Socket 中，而是将它对于的数据描述信息（内存地址、文件描述符，文件长度等）记录到 Socket 缓冲区中，最后由 DMA 根据这些文件描述信息从内核读缓冲区中找到数据，直接拷贝到网卡设备中。
- **splice**：splice 系统调用可以在内核空间的读缓冲区和网络缓冲区之间建立管道（pipeline），从而避免了两者之间的 CPU 拷贝操作。



---



## 立项依据

本项目的立项依据是分布式文件系统中 存在**IO路径过长**、**数据需要多次写入**的现状和 **eBPF 技术可以绕过内核进行数据拷贝**的优势。

首先，传统的分布式文件系统通常需要不断进行用户态和内核态切换进行文件拷贝，需要将硬盘数据先拷贝到内核缓冲区以及将内核缓冲区数据拷贝到用户缓冲区等多次拷贝操作，这在大量进行文件访问时会带来很大时间开销。

其次，eBPF 技术在解决 IO 性能问题方面具有独特的优势。eBPF 技术可以绕过内核进行一些文件操作，减少了用户态和内核态切换的时间开销，同时也能减少多次拷贝的时间开销。

因此，本项目旨在借助 eBPF 技术，实现分布式文件系统的 IO 性能优化，提高文件系统的文件访问传输性能。

---



## 前瞻性/重要性分析

本项目的前瞻性和重要性在于：

1. IO 性能优化是分布式文件系统的关键问题之一，传统I/O 操作，内存与磁盘的数据传输的工作都是由 CPU 完成的，而此时 CPU 不能执行其他任务，会特别浪费 CPU 资源。并且，传统的文件系统存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。
2. eBPF 技术在解决 IO 性能问题方面具有独特的优势， 利用eBPF 技术进行优化，减少用户态和内核态的上下文切换，减少拷贝次数，可以提高CPU利用率，减少时间开销，同时由于内核eBPF校验器的存在，使得文件访问传输的可靠性和安全性也有一定的保证。

当今时代，随着数据量的不断增加，文件系统I/O性能的提高变得越来越重要。这是因为文件系统I/O性能的提高可以带来更快的读写速度，从而提高系统的整体性能。因此，本项目的研究具有重要的前瞻性和应用价值。通过本项目的研究，可以深入探索eBPF技术在分布式文件系统中的应用，为提高系统的IO性能提供有效的解决方案，为未来的分布式系统的发展做出贡献。

---



## 相关工作

### [ExtFUSE](https://zhuanlan.zhihu.com/p/518531067)(使用ebpf优化FUSE性能)

0. ##### FUSE简介: 

   implementing filesystems in user space，在用户空间实现文件系统。简单讲，用户可通过fuse在用户空间来定制实现自己的文件系统。fuse实现了一个对文件系统访问的回调。fuse分为内核态的模块和用户态的库两部分。其中用户态的库为程序开发提供接口，也是我们实际开发时用的接口，我们通过这些接口将请求处理功能注册到fuse中。内核态模块是具体的数据流程的功能实现，它截获文件的访问请求，然后调用用户态注册的函数进行处理。

1. ##### 项目简介：

   使用ebpf的方式通过预先缓存ionde和dentry降低元数据操作的延迟，访问元数据的操作直接在内核中返回，而不需要把该操作发送到FUSE的用户态服务中去。

2. ##### 原理：

![img](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/v2-521eddbda0732b622ab9ae91747b58b2_720w.webp)

Extfuse的实现框架图如上所示，它通过在FUSE的内核与用户层之间通讯fuse_request_send函数中增加extfuse的ebpf钩子来实现ebpf程序的挂载。ebpf的程序通过判断文件或者目录的内容是否在cache中，如果在cache中则请求直接返回，而不用访问用户态文件系统的相应接口。而cache是采用ebpf框架中的BPF_MAP_TYPE_HASH，这个数据结构可以在用户态和内核态之间共享和查询数据。

Extfuse通过hook FUSE传到用户态的元数据操作，发现cache元数据操作需要的inode或者dentry的内容时，就直接返回。它主要截获的元数据操作有FUSE_LOOKUP, FUSE_GETATTR，FUSE_SETATTR，FUSE_RENAME，FUSE_RMDIR。

3. ##### 优化效果：

![img](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/v2-1e9909c12970b0e9e2be68f86d4c580f_720w.webp)

优化的结果如上图所示。在图中显示了对内核做编译操作和解压缩操作的测试情况下监测到的应用层操作的对比。文件系统使用的是作者开发的用户态文件系统StackFS，AllOpt表示对READ和WRITE操作也在内核中进行了直接返回，而不发到应用层。MDOpt操作表示只是对元数据在内核中直接返回，Opt是原始的FUSE，不做ebpf的优化。

在内核的编译测试中，可以看到lookup，getxattr，getattr的用户态操作都降低了很多。在内核的解压缩测试中，可以看到需要在用户态处理的getattr，getxattr，lookup元数据操作明显减少。

4. ##### 具体实现：

a. linux内核增加extfuse的ebpf的程序类型；

b.  在FUSE的内核中增加ebfp的挂载点以及相应的钩子函数，并增加辅助函数；

c.  设计相应的ebpf挂载函数；

d.  在用户态文件系统建立与内核共同使用的ebpf map，并在相关的元数据操作中维护ebpf map。

### [利用 eBPF 优化 K8s Service ](https://www.cnblogs.com/tencent-cloud-native/p/13566340.html)

0. ##### K8s(kubernetes)简介：

   Kubernetes 也称为 K8s，是用于自动部署、扩缩和管理容器化应用程序的开源系统。

service是kubernetes中最核心的资源对象之一，service和pod之间是通过Label串起来,相同的Service的pod的Label是一样的。 同一个service下的所有pod是通过kube-proxy实现负载均衡.而每个service都会分配一个全局唯一的虚拟ip,也就cluster ip。Service可以看作是一组提供相同服务的Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡

1. ##### 优化方案

针对nf_conntrack带来的性能问题，腾讯TKE团队设计实现了IPVS-BPF。核心思想是绕过nf_conntrack，减少处理每个报文的指令数目，从而节约CPU，提高性能。其主要逻辑如下:

a. 在IPVS内核模块中引入开关，支持原生IPVS逻辑和IPVS-BPF逻辑的切换

b. 在IPVS-BPF模式下，将IPVS hook点从LOCALIN前移到PREROUTING，使访问service的请求绕过nf_conntrack

c. 在IPVS新建连接和删除连接的代码中，相应的增删eBPF map中的session信息

d. 在qdisc挂载eBPF的SNAT代码，根据eBPF map中的session信息执行SNAT

优化前后流程对比：

![ 'bpf-process.png'](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/2041406-20200826172233366-1601111089.png)

2. ##### 性能测试

通过量化分析的方法，用perf工具读取CPU性能计数器，从微观的角度解释宏观的性能数据。这里采用的压测程序是wrk和iperf。

![ 'nodeport短连接cps.png'](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/2041406-20200826172236072-1062852469.png)

![ 'clusterip短连接cps.png'](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/2041406-20200826172236857-756339901.png)

IPVS-BPF模式相对IPVS模式，Nodeport短连接性能提高了64%，clusterIP短连接性能提高了40%。

NodePort优化效果更明显，是因为NodePort需要SNAT，而eBPF SNAT比iptables SNAT更高效，所以性能提升更多。

指令数与CPI:

![ 'nodeport-inst-per-req.png'](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/2041406-20200826172239418-304165824.png)

![ 'nodeport-cpi.png'](./%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A_ys.assets/2041406-20200826172240088-2036361396.png)

上图中从Perf工具看，IPVS-BPF 模式下CPI略有增加，大概16%，但平均每个请求耗费的CPU指令数, IPVS-BPF模式下降了38%。这也是性能提升的最主要原因。

---





## 可行性

该项目具有较高的可行性。首先参考2021级OSH项目[x-DisGraFS](https://github.com/OSH-2021/x-DisGraFS)，我们可以在DisGraFS的基础上改善分布式文件系统的IO机制，并且保证部署的稳定性和便捷性，以期望提高分布式文件系统的性能。同时借鉴2022级OSH项目对DisGraFS的[监控]([GitHub - OSH-2022/x-TOBEDONE: team TOBEDONE in USTC-OSH-2022](https://github.com/OSH-2022/x-TOBEDONE))以及[完善DisGraFS文件系统的缓存机制]([GitHub - OSH-2022/x-WowKiddy: 2022 USTC OSH project](https://github.com/OSH-2022/x-WowKiddy))的部署，更好地完成前期的部署和后期的测试工作。其次，eBPF是一个相对成熟的技术，在安全性和可靠性方面都有一定的保证。最后，目前已有一些eBPF进行性能优化的相关项目可以参考，也可以确认eBPF在性能优化领域具有一定的实战意义。

实现分布式文件系统IO效率优化的基本思路是利用eBPF技术，避免用户态与内核态的多次切换以及数据在用户空间和内核空间之间的拷贝，从而提高文件传输的效率。具体的实现思路可以分为以下几个步骤：

1. 部署DisGraFS文件系统，深入了解DisGraFS内部的实现逻辑和底层原理以便后续对其IO进行优化

2. 设计合适的eBPF程序，用于实现零拷贝技术。可以使用eBPF提供的数据结构eBPF map和eBPF的辅助函数，实现绕过操作系统内核直接进行文件访问与传输的功能。

3. 进行实验验证。设计合适的测试用例，验证零拷贝技术和算法策略的有效性和优劣性。

以上是实现分布式文件系统IO效率优化的基本思路。当然，具体的实现细节还需要根据具体的情况进行调整和优化。

同时，该项目还存在一些技术挑战。例如，如何在保证IO性能的同时，保证数据的安全性和一致性；如何将eBPF技术与其他技术（如RDMA等）结合起来，实现更好的性能优化；如何针对不同类型的应用场景进行优化等。这些问题需要在项目开发过程中得到充分的考虑和解决。

综上，该项目具有一定的可行性，并且在开发过程中也存在一些技术挑战，需要充分的研究和解决。

---



## 参考资料：

[eBPF 科普第一弹｜ 初识 eBPF，你应该知道的知识_基础软件_Daocloud 道客_InfoQ写作社区](https://xie.infoq.cn/article/a03bff551844957cc79a4c92d#:~:text=eBPF)

[使用ebpf优化FUSE的性能 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/518531067)

[性能提升40%: 腾讯 TKE 用 eBPF 绕过 conntrack 优化 K8s Service - 腾讯云原生 - 博客园 (cnblogs.com)](https://www.cnblogs.com/tencent-cloud-native/p/13566340.html)

[(16条消息) kubernetes(k8s) ：service实现服务发现和负载均衡_鲸鱼妹子‍的博客-CSDN博客](https://blog.csdn.net/weixin_43936969/article/details/106137347)

[(16条消息) 谈谈分布式系统中的复制_数据复制是构成分布式系统,尤其是分布式存储和分布式数据库的关键所在,其中常见的_jiafu1115的博客-CSDN博客](https://blog.csdn.net/jiafu1115/article/details/53908715)

[面试-操作系统篇：文件系统与I/O - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/345282009)

[原来 8 张图，就可以搞懂「零拷贝」了 - 小林coding - 博客园 (cnblogs.com)](https://www.cnblogs.com/xiaolincoding/p/13719610.html)](https://zhuanlan.zhihu.com/p/518531067)
