# 技术路线

一、如何实现单机文件系统io？

1. 使用ebpf技术，通过在内核中挂载ebpf程序，实现对文件系统io的跟踪和统计，获取每个io从文件到落盘的各个阶段的事件、时间戳、pid、io类型等信息。
2. 使用ebpf提供的hash map数据结构，建立inode cache，降低元数据操作的开销，用户态文件系统可以通过统计和预测应用的行为来预加载inode cache到VFS中，提升文件系统的元数据操作性能。
3. 使用ebpf提供的ring buffer数据结构，将内核中统计和缓存的io数据传递到用户态，避免使用文件输出方式造成的额外开销和解析难度。
4. 使用ebpf提供的tracepoint和kprobe机制，可以在不修改内核代码或加载内核模块的情况下，动态地对内核函数或系统调用进行hook和注入，实现对io栈的定制化优化。

二、如何评估单机io性能优化效果？

1. 使用fio工具，对不同类型和大小的文件进行读写测试，比较使用ebpf优化前后的吞吐量、延迟、CPU占用等指标，分析ebpf优化对不同场景下的io性能的影响。
2. 使用perf工具，对内核中各个函数或模块的执行时间和调用次数进行统计，比较使用ebpf优化前后的内核开销分布，分析ebpf优化对内核资源消耗的影响。
3. 使用bpftrace或bcc工具，对内核中各个阶段的io事件进行可视化展示，比较使用ebpf优化前后的io路径和流程，分析ebpf优化对io栈的改造程度。

三、单机io做好后，多点之间的io如何实现？

1. 使用FUSE技术，将用户态文件系统挂载到VFS中，实现对远程存储资源的访问和管理。
2. 使用ebpf技术，在FUSE的内核与用户层之间通信函数中增加ebfp的挂载点和钩子函数，并加载相应的ebpf程序，实现对FUSE操作的跟踪和优化。
3. 使用ebpf提供的网络事件hook point，实现对网络层协议栈的监测和调整，提高网络传输效率和稳定性。
4. 使用ebpf提供的XDP机制，在网卡驱动层进行数据包处理，实现对网络流量的过滤和重定向，减少网络层开销和延迟。
