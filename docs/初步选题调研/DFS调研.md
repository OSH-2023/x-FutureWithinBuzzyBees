# 分布式文件系统方向选题调研



## 分布式文件系统概述

### 目前主流分布式文件系统

*删去的为未开源的*

- ~~GFS(Google)~~、HDFS（架构与GFS类似，采用java实现，还没有成熟的C++开源版本）
- HDFS
- Ceph
- Lustre
- MogileFS
- MooseFS：C实现
- FastDFS：C实现
- TFS
- GridFS

- GlusterFS：开源分布式横向扩展文件系统
- JuiceFS：使用公有云中已有的对象存储来替换 DataNode 和 ChunkServer

![image-20230322013854948](DFS调研.assets\image-20230322013854948.png)

### 相关名词

#### POSIX

可移植操作系统接口

### 按特性分类

|                        | FastDFS | GlusterFS | GridFS | Ceph | Lustre | MooseFS | GlusterFS | MogileFS |
| ---------------------- | ------- | --------- | ------ | ---- | ------ | ------- | --------- | -------- |
| 适合做通用文件系统     |         | √         |        | √    | √      | √       |           |          |
| 适合小文件存储         | √       |           |        | √    |        | √       |           | √        |
| 适合大文件存储         |         | √         | √      | √    | √      |         |           |          |
| 轻量级文件系统         | √       |           |        |      |        | √       |           |          |
| 简单易用，用户数量活跃 | √       | √         |        |      |        | √       |           | √        |
| 支持FUSE挂载           |         | √         |        | √    | √      | √       |           |          |

### FastDFS

基于互联网应用的开源分布式文件系统，主要用于大中型网站存储资源文件，如图片、文档、音频、视频等。

跟踪服务器（tracker server）+存储服务器（storage server）+客户端（client）

适用中小文件，建议4KB-500MB

解决了大容量的文件存储和高并发访问问题

文件存取实现了负载均衡

采用类似GFS的架构，用纯C语言实现，支持Linux、FreeBSD、AIX等UNIX 系统。

用户端只能通过专有API对文件进行存取访问，不支持POSIX接口方式。

GFS以及 FastDFS、mogileFS、HDFS、TFS等类GFS系统都**不是系统级的分布式文件系统**，而是**应用级的分布式文件存储服务**。

缺点：

- 不支持**断点续传**，**不适合大文件存储**；
- 不支持POSIX，通用性较低；
- 对跨公网的文件同步，存在较大延迟，需要应用做相应的容错策略；
- **同步机制不支持文件正确性校验**；
- 通过API下载，存在单点的性能瓶颈

- 写一份即成功：从源storage写完文件至同步到组内其他storage的时间窗口内，一旦源storage出现故障，就可能导致用户数据丢失，而数据的丢失对存储系统来说通常是不可接受的。

- 缺乏自动化恢复机制：当storage的某块磁盘故障时，只能换存磁盘，然后手动恢复数据；由于按机器备份，似乎也不可能有自动化恢复机制，除非有预先准备好的热备磁盘，缺乏自动化恢复机制会增加系统运维工作。
- 数据恢复效率低：恢复数据时，只能从group内其他的storage读取，同时由于小文件的访问效率本身较低，按文件恢复的效率也会很低，低的恢复效率也就意味着数据处于不安全状态的时间更长。
- 缺乏多机房容灾支持：目前要做多机房容灾，只能额外做工具来将数据同步到备份的集群，无自动化机制。

存储空间利用率

- 单机存储的文件数受限于inode数量
- 每个文件对应一个storage本地文件系统的文件，平均每个文件会存在block_size/2的存储空间浪费。
- 文件合并存储能有效解决上述两个问题，但由于**合并存储没有空间回收机制**，删除文件的空间不保证一定能复用，也存在空间浪费的问题

负载均衡

- group机制本身可用来做负载均衡，但这只是一种静态的负载均衡机制，需要预先知道应用的访问特性；同时group机制也导致不可能在group之间迁移数据来做动态负载均衡。

文件去重：

FastDFS不具备文件去重能力，必须引入FastDHT 来配合完成。FastDHT 是一个键值对的高效分布式hash系统，底层采用Berkeley DB 来做数据库持久化，同步方式使用binlog复制方式。在FastDFS去重场景中，对文件内容做hash，然后判断文件是否一致。

在文件上传成功后，查看 Storage存储对应存储路径，会发现返回的是一个软链接，之后每次重复上传都是返回一个指向第一次上传的文件的软链接。也就保证了文件只保存了一份。

（注意：FastDFS不会返回原始文件的索引，返回的全部都是软链接，当所有的软链接都被删除的时候，原始文件也会从FastDFS中被删除）。

### Ceph

- 分层结构

  底层是一个基于CRUSH（哈希）的分布式对象存储，上层提供对象存储（RADOSGW）、块存储（RDB）和文件系统（CephFS）三个API

- 整体结构：

<img src="DFS调研.assets\image-20230322013540253.png" alt="image-20230322013540253" style="zoom:80%;" />

### 看到的一些已有选题

#### 基于FastDFS+Redis的分布式文件存储系统及方法

![image-20230322015458379](DFS调研.assets\image-20230322015458379.png)

##### Redis

使用ANSI C编写的开源、包含多种数据结构、支持网络、基于内存、可选持久性的键值对存储数据库

Redis提供的数据类型主要分为5种自有类型和一种自定义类型，这5种自有类型包括：String类型、哈希类型、列表类型、集合类型和顺序集合类型。

特点：

- 基于内存运行，性能高效
- 支持分布式，理论上可以无限扩展
- key-value存储系统
- 开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API

##### 背景

推动着大数据时代的到来，结构化数据也在以爆炸式的速度增长。结构化数据通常是指大小处于2KB-1MB之间的小文件。文件数据的存储包含文件元数据和文件内容的存储，目前文件元数据的存储通常采用MySQL，数据规模越来越大时存取性能急剧下降；FastDFS作为一个轻量级的开源分布式文件系统，适合中小型文件的存储，但FastDFS将数据存储在磁盘上，每次访问都需要从磁盘读取数据，多次的IO造成了查询性能的降低。

为减小磁盘的访问次数，提高文件的读速度，可选的方案有引入Redis作为文件缓存服务，但Redis是基于内存的数据库，直接利用Redis缓存文件会造成内存空间占用过大、查询性能低的问题，从而无法适用于大规模文件数据。且使用Redis作为缓存服务时，Redis提供的6种原生缓存淘汰策略，对于随意性和周期性的查询，缓存命中率不高。

旨在实现文件数据更为高效的存储和读写，解决Redis缓存文件时字符串长度过长、内存空间占用大的问题，提高了内存空间的利用率，具备良好的缓存命中率。

##### 功能与实现思路

文件上传：

1：根据指定路径获取文件内容和文件元数据信息，将文件上传到FastDFS；

2：判断文件上传是否成功，若失败则抛出异常，结束；

3：将上传成功后返回的FileID与元数据写入Redis；

4：判断Redis写数据是否成功，若成功，则文件上传成功；

5：判断写Redis次数是否小于系统默认设置值，若小于转步骤3；

6：从FastDFS中删除文件，并返回上传失败。

缓存压缩：

1：待缓存文件使用Base64编码；

2：对编码后的文件使用Gzip算法压缩；

3：将编码压缩后的文件写入Redis。

查询模块：

1：根据查询条件，先查询Redis，若Redis中存在该文件的缓存，则返回经Gzip算法解压和Base64解码后的文件；

2：否则，利用FileID查询FastDFS获取文件，更新记录的查询次数，判断是否符合缓存替换算法的执行条件。

缓存替换：

1：使用一个历史队列记录文件的使用次数；

2：将文件索引写入历史访问队列，若该文件的索引已经存在，则对该记录的使用次数加1；若不存在，则将该文件索引写入访问队列中，并且设置使用次数为1；

3：当文件使用次数达到k次时，将编码压缩后的文件写入Redis缓存中；

4：判断缓存容量是否小于系统预设的阈值，若是，转步骤6；

5：按照文件使用频率高低，进行缓存的淘汰选择；

6：将编码压缩后的文件数据写入Redis。

删除：

1：根据删除条件查询Redis，获取文件的FileID；

2：判断Redis中是否存在文件的缓存，存在则转步骤6；

3：根据FileID，删除FastDFS上保存的文件；

4：判断文件删除是否成功，成功则结束；

5：判断尝试删除次数是否小于系统预设的阈值，是则删除次数加一，转步骤3；否则结束；

6：拷贝文件作为副本；

7：执行同时删除的操作，分别删除Redis上的缓存和FastDFS上的文件；

8：判断是否同时删除成功，是则删除成功，结束；

9：判断尝试删除次数是否小于系统预设的阈值，是则删除次数加一，转步骤7；

10：回滚删除操作，将预先保存的副本重新写入中间件中，结束。

#### 群智感知环境下的分布式文件系统的设计与实现

现有的分布式文件系统大多采用集中式的元数据管理策略，集中式的元数据管理策略不适用于群智感知环境下海量的小文件产生的海量元数据量。群智感知要求分布式文件系统能高效地支持大小文件混存。设计并实现了一个分布式元数据服务器的、同时支持大小文件存储的分布式文件系统。

大文件使用多个64MB的数据块来存放，多个小文件共用一个数据块并通过索引文件维护小文件在数据块的位置。

为了让文件系统能适应不断变化的负载，设计了基于遗传算法的元数据服务器间负载均衡算法，并通过实验对负载均衡算法进行了验证。

> FastDFS中的存储策略
>
> 5.1 LOSF问题
>
> 小文件存储（LOSF）面临的问题：
>
> 本地文件系统innode梳理优先，存储小文件数量受限。
>
> 目录层级和目录中文件数量会导致访问文件开销很大（IO次数多）。
>
> 小文件存储，备份和恢复效率低。
>
> - 针对小文件存储问题，FastDFS 提供了文件合并解决方案。FastDFS 默认创建大文件为 64M，大文件可以存储很多小文件，容纳一个小文件的空间叫slot，solt 最小256字节，最大16M。小于256字节当256字节存储，超过16M文件单独存储。
> - 存储方式
>   - 默认存储方式：未开启合并，FastDFS生成的file_id 和磁盘上实际存储的文件一一对应。
>   - 合并存储方式：多个file_id对应文件被存储成了一个大文件 。trunk文件名格式：/fastdfs/data/00/000001 文件名从1开始递增。而生成的file_id 更长，会新增16个字节额外内容用来保存偏移量等信息。
> - 存储空间管理
>   - **【Trunk Server】**由tracker leader 在一组Storage Server 选择出来的，并通知给该组内所有Storage Server，负责为该组内所有upload操作分配空间。
>   - **【空闲平衡树】**trunk server 会为每个store_path构造一个空闲平衡树，相同大小的空闲块保存在链表中，每次上传请求时会到根据上传的文件大小到平衡树中查找获取大于或者接近的空闲块，然后从空闲块中分割出多余的作为新的空闲块，重新加入平衡树。如果找不到则会重建一个新的trunk文件，并加入到平衡树中。该分配过程即是一个维护空闲平衡树的过程。
>   - **【Trunk Binlog】**开启了合并存储后，Trunk Server 会多出一个TrunkBinlog同步。TrunkBinlog记录了TrunkServer 所有分配与回收的空闲块操作，并由Trunk Server同步给同组中其他storage server。
>   - 

#### 元数据管理系统相关

1. 针对多节点上的元数据分布问题，提出一种基于目录子树迁移与复制的动态元数据管理策略。

   通过感知服务器负载状况，根据负载均衡策略选择从负载重的元数据服务器上选择相应的目录子树，并通过迁移和复制策略将过热目录转移或复制到其他负载较轻的元数据服务器上管理，这种策略不仅完成了元数据的动态分布工作，而且可以很好的达到服务器间负载平衡。

   *负载均衡在Fast中已经有了相关实现*

2. 针对多节点情况下的元数据副本一致性问题，实现元数据副本管理机制。

   该策略主要包括元数据主从副本的架构设计,以及两阶段提交协议与多状态转换机制结合的锁的机制。该机制通过多状态锁机制保证多副本的更新一致性。多状态的FileLock类型锁还解决用户对文件的读写互斥访问问题。

3. 针对元数据的可靠性问题,设计并实现一种元数据备份机制。

   该机制采用了元数据异步提交、本地化与备份机存储结合的方法。元数据提交时通过异步过程，将元数据信息按照存储基本单元存储到本地数据库中，然后将数据存储到其他备份元数据服务器上，从而达到数据备份的可靠性要求。